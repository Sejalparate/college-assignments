{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename):\n",
    "    with open(filename, 'rb') as pdf_file:\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    text = text.lower()\n",
    "\n",
    "    filtered_text = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sejbp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pdf_text = extract_text_from_pdf(\"Corpus.pdf\")\n",
    "cleaned_text = clean_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = create_ngrams(cleaned_text, 1)\n",
    "bigrams = create_ngrams(cleaned_text, 2)\n",
    "trigrams = create_ngrams(cleaned_text, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_ngrams(ngrams, vocab_size):\n",
    "    smoothed_probs = {}\n",
    "    for ngram in ngrams:\n",
    "        count = ngrams.count(ngram)\n",
    "        smoothed_probs[ngram] = (count + 1) / (vocab_size + len(ngrams))\n",
    "    return smoothed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(unigrams)  # Get vocabulary from unigrams\n",
    "smoothed_bigrams = smooth_ngrams(bigrams, len(vocab))\n",
    "smoothed_trigrams = smooth_ngrams(trigrams, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(prefix, smoothed_bigrams, smoothed_trigrams, vocab):\n",
    "    # Check if trigram exists\n",
    "    if prefix in smoothed_trigrams:\n",
    "        next_word_probs = {word: smoothed_trigrams[f\"{prefix} {word}\"] for word in vocab}\n",
    "        return max(next_word_probs, key=next_word_probs.get)  # Get word with highest probability\n",
    "\n",
    "    # If trigram not found, fallback to bigram\n",
    "    elif len(prefix.split()) == 2 and prefix in smoothed_bigrams:\n",
    "        next_word_probs = {word: smoothed_bigrams[f\"{prefix} {word}\"] for word in vocab}\n",
    "        return max(next_word_probs, key=next_word_probs.get)  # Get word with highest probability\n",
    "\n",
    "    # If no match in trigrams or bigrams, return a random word from the vocabulary\n",
    "    else:\n",
    "        return random.choice(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word for 'There is a beautiful mountain and': background\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prefix = \"There is a beautiful mountain and\"  # Bigram prefix\n",
    "predicted_word = predict_next_word(prefix, smoothed_bigrams, smoothed_trigrams, vocab)\n",
    "print(f\"Predicted next word for '{prefix}': {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
